{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:07.113614Z",
     "start_time": "2023-10-22T20:02:07.105584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:07.113873Z",
     "start_time": "2023-10-22T20:02:07.108614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:07.114008Z",
     "start_time": "2023-10-22T20:02:07.111222Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:07.124688Z",
     "start_time": "2023-10-22T20:02:07.114075Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('antoinelouis/biencoder-mMiniLMv2-L12-mmarcoFR')\n",
    "model = AutoModel.from_pretrained('antoinelouis/biencoder-mMiniLMv2-L12-mmarcoFR')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.685578Z",
     "start_time": "2023-10-22T20:02:07.117555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.689670Z",
     "start_time": "2023-10-22T20:02:09.688086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def get_embeddings(chunck, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the embedding of a passage\n",
    "    :param chunck: the passage\n",
    "    :param model: the model\n",
    "    :param tokenizer: the tokenizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(chunck, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    chunk_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return chunk_embeddings.view(-1).numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.693927Z",
     "start_time": "2023-10-22T20:02:09.689211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    To read the file\n",
    "    :param filename: string the name of the file\n",
    "    :return: string  the content of the file\n",
    "    \"\"\"\n",
    "    with open(f'judilibre_json/data/{filename}') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.694035Z",
     "start_time": "2023-10-22T20:02:09.691837Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.738141Z",
     "start_time": "2023-10-22T20:02:09.694817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.738271Z",
     "start_time": "2023-10-22T20:02:09.736768Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "def split_text_into_passages(text,tokenizer, prev_include=2):\n",
    "    \"\"\"\n",
    "    Split text into passage with maximum number of char per passage and number of char of the previous\n",
    "    passage to include in the following\n",
    "    :param text:  string the text to split\n",
    "    :param tokenizer: tokenizer utilise\n",
    "    :param prev_include: int number of char of the previous to include in the following\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    current_passage = []\n",
    "    passage_ends = []\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        words = line.split()\n",
    "        for j, word in enumerate(words):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            current_passage_tokens = tokenizer.tokenize(\" \".join(current_passage))\n",
    "            if len(current_passage_tokens) + len(word_tokens) <= tokenizer.model_max_length:\n",
    "                current_passage.append(word)\n",
    "            else:\n",
    "                passages.append(\" \".join(current_passage))\n",
    "                passage_ends.append(i + 1)\n",
    "                if prev_include > 0:\n",
    "                    current_passage = current_passage[-prev_include:]\n",
    "                else:\n",
    "                    current_passage = []\n",
    "\n",
    "                current_passage.append(word)\n",
    "    if current_passage:\n",
    "        passages.append(\" \".join(current_passage))\n",
    "        passage_ends.append(len(lines))\n",
    "\n",
    "    chunks = list(range(1, len(passages) + 1))\n",
    "\n",
    "    df = pd.DataFrame({'chunck': chunks, 'line': passage_ends, 'passage': passages})\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.743654Z",
     "start_time": "2023-10-22T20:02:09.740221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def get_all_tsv_file():\n",
    "    \"\"\"\n",
    "    Get the name of all data file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    path = 'judilibre_json/data'\n",
    "    tsv_files = []\n",
    "    if os.path.exists(path) and os.path.isdir(path):\n",
    "        tsv_files = [f for f in os.listdir(path) if f.endswith(\".tsv\")]\n",
    "    return tsv_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.744864Z",
     "start_time": "2023-10-22T20:02:09.742575Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def generate_passages(tokenizer, prev_include=2):\n",
    "    all_tsv_files = get_all_tsv_file()\n",
    "    all_dfs = []\n",
    "\n",
    "    for filename in tqdm(all_tsv_files, total=len(all_tsv_files), desc=\"Generating Passages\", unit=\" file\"):\n",
    "        id_dec, _ = os.path.splitext(filename)\n",
    "        text = read_file(filename)\n",
    "        df = split_text_into_passages(text, tokenizer, prev_include)\n",
    "        n_rows, _ = df.shape\n",
    "        df.insert(0, 'id_dec', [id_dec] * n_rows)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    df_judilibre_v = pd.concat(all_dfs, ignore_index=True)\n",
    "    df_judilibre_v.to_csv(f'judilibre_v/judilibre_v_{len(all_tsv_files)}_fichiers.tsv', index=False)\n",
    "\n",
    "    return df_judilibre_v"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T20:02:09.753813Z",
     "start_time": "2023-10-22T20:02:09.745806Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Passages:  27%|██▋       | 30871/113426 [2:51:31<8:13:38,  2.79 file/s] Token indices sequence length is longer than the specified maximum sequence length for this model (134 > 128). Running this sequence through the model will result in indexing errors\n",
      "Generating Passages: 100%|██████████| 113426/113426 [10:32:03<00:00,  2.99 file/s]  \n"
     ]
    }
   ],
   "source": [
    "df_passages = generate_passages(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T06:34:47.685951Z",
     "start_time": "2023-10-22T20:02:09.748947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def generate_embeddings(df, model, tokenizer):\n",
    "    n_rows, _ = df.shape\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(n_rows), desc=\"Generating Embeddings\", unit=\" passage\"):\n",
    "        chunk = df['passage'][i]\n",
    "        embedding = get_embeddings(chunk, model, tokenizer)\n",
    "        embeddings.append(embedding)\n",
    "    df['embedding'] = embeddings\n",
    "    df.to_csv(f'judilibre_v/judilibre_v.tsv', index=False)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T06:34:47.711542Z",
     "start_time": "2023-10-23T06:34:47.707725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "#df_embeddings = generate_embeddings(df_passages, model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T06:34:47.712107Z",
     "start_time": "2023-10-23T06:34:47.708579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "                                   id_dec  chunck  line  \\\n0        JURITEXT63c8eef7dc5b777c90992fb9       1    10   \n1        JURITEXT63c8eef7dc5b777c90992fb9       2    16   \n2        JURITEXT63c8eef7dc5b777c90992fb9       3    25   \n3        JURITEXT63c8eef7dc5b777c90992fb9       4    37   \n4        JURITEXT63c8eef7dc5b777c90992fb9       5    40   \n...                                   ...     ...   ...   \n4215708  JURITEXT6253ccdcbd3db21cbdd9185b      30    83   \n4215709  JURITEXT6253ccdcbd3db21cbdd9185b      31    85   \n4215710  JURITEXT6253ccdcbd3db21cbdd9185b      32    88   \n4215711  JURITEXT6253ccdcbd3db21cbdd9185b      33    93   \n4215712  JURITEXT6253ccdcbd3db21cbdd9185b      34    99   \n\n                                                   passage  \n0        COUR D'APPEL D'ORLÉANS Chambre des référés - P...  \n1        REPASS'CHIC prise en la personne de Me Delphin...  \n2        deFatima HAJBI, greffier, Statuant en référé d...  \n3        [E] 120 allée du Séquoia 45770 SARAN ni compar...  \n4        novembre 2022 L'avis du Ministère public a été...  \n...                                                    ...  \n4215708  d'une prise d'acte de la rupture du contrat de...  \n4215709  être fixée qu'en fonction du préjudice subi pa...  \n4215710  prise d'acte de la rupture du contrat de trava...  \n4215711  une prise d'acte, et a condamné la Société ALM...  \n4215712  suivantes : -775, 61 euros au titre des heures...  \n\n[4215713 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_dec</th>\n      <th>chunck</th>\n      <th>line</th>\n      <th>passage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JURITEXT63c8eef7dc5b777c90992fb9</td>\n      <td>1</td>\n      <td>10</td>\n      <td>COUR D'APPEL D'ORLÉANS Chambre des référés - P...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>JURITEXT63c8eef7dc5b777c90992fb9</td>\n      <td>2</td>\n      <td>16</td>\n      <td>REPASS'CHIC prise en la personne de Me Delphin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JURITEXT63c8eef7dc5b777c90992fb9</td>\n      <td>3</td>\n      <td>25</td>\n      <td>deFatima HAJBI, greffier, Statuant en référé d...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>JURITEXT63c8eef7dc5b777c90992fb9</td>\n      <td>4</td>\n      <td>37</td>\n      <td>[E] 120 allée du Séquoia 45770 SARAN ni compar...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>JURITEXT63c8eef7dc5b777c90992fb9</td>\n      <td>5</td>\n      <td>40</td>\n      <td>novembre 2022 L'avis du Ministère public a été...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4215708</th>\n      <td>JURITEXT6253ccdcbd3db21cbdd9185b</td>\n      <td>30</td>\n      <td>83</td>\n      <td>d'une prise d'acte de la rupture du contrat de...</td>\n    </tr>\n    <tr>\n      <th>4215709</th>\n      <td>JURITEXT6253ccdcbd3db21cbdd9185b</td>\n      <td>31</td>\n      <td>85</td>\n      <td>être fixée qu'en fonction du préjudice subi pa...</td>\n    </tr>\n    <tr>\n      <th>4215710</th>\n      <td>JURITEXT6253ccdcbd3db21cbdd9185b</td>\n      <td>32</td>\n      <td>88</td>\n      <td>prise d'acte de la rupture du contrat de trava...</td>\n    </tr>\n    <tr>\n      <th>4215711</th>\n      <td>JURITEXT6253ccdcbd3db21cbdd9185b</td>\n      <td>33</td>\n      <td>93</td>\n      <td>une prise d'acte, et a condamné la Société ALM...</td>\n    </tr>\n    <tr>\n      <th>4215712</th>\n      <td>JURITEXT6253ccdcbd3db21cbdd9185b</td>\n      <td>34</td>\n      <td>99</td>\n      <td>suivantes : -775, 61 euros au titre des heures...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4215713 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_passages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T10:46:40.234561Z",
     "start_time": "2023-10-23T10:46:40.216772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
